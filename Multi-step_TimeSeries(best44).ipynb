{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras.backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    " \n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from collections import Counter\n",
    "from numpy import concatenate\n",
    "\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the monthly df from daily df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# da = pd.read_csv('Whse_J_sort_out_index.csv')\n",
    "# da.columns.values[0] = 'Date'\n",
    "# da.index = pd.to_datetime(da['Date'])\n",
    "# da.drop('Date', axis=1,inplace=True)\n",
    "#取完整的年當資料集\n",
    "# da = da[(da.index.year >=2012) &(da.index.year <=2016)]\n",
    "# da['Year_Month'] = da.index.to_period('M')\n",
    "# df = da.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#generate new  df for experiment\n",
    "# index1 = pd.date_range('2012-1', '2017-1' ,freq='M').to_period('M')\n",
    "# column1 = df.columns.to_list()\n",
    "# column1 = column1[0:-1]\n",
    "# np1 = np.zeros((len(index1), len(column1)))\n",
    "# df_test = pd.DataFrame(np1,index=index1,columns=column1,dtype='int64')\n",
    "# df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test.loc[df_ym[i]][j]\n",
    "# df[df.Year_Month == df_test.index[0]].iloc[:-1,4].sum()\n",
    "# df[df.Year_Month == df_test.index[1]].iloc[:-1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #generate monthly df from daily df\n",
    "# df_ym = df.index.to_period('M').unique()\n",
    "# dftest_len = len(df_ym)\n",
    "# for i in range(dftest_len):\n",
    "#     k=0\n",
    "#     for j in column1:\n",
    "#         df_test.loc[df_ym[i]][j]=df[df.Year_Month == df_test.index[i]].iloc[:-1,k].sum()\n",
    "#         k+=1\n",
    "# # df_test.to_csv('Whse_J_sort_out_Month_aggressive.csv')\n",
    "# df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category_007-Product_0138</th>\n",
       "      <th>Category_021-Product_0282</th>\n",
       "      <th>Category_006-Product_0107</th>\n",
       "      <th>Category_001-Product_0609</th>\n",
       "      <th>Category_007-Product_0322</th>\n",
       "      <th>Category_005-Product_0031</th>\n",
       "      <th>Category_019-Product_1228</th>\n",
       "      <th>Category_024-Product_0289</th>\n",
       "      <th>Category_021-Product_0277</th>\n",
       "      <th>Category_024-Product_1898</th>\n",
       "      <th>...</th>\n",
       "      <th>Category_007-Product_0241</th>\n",
       "      <th>Category_007-Product_0242</th>\n",
       "      <th>Category_007-Product_0238</th>\n",
       "      <th>Category_007-Product_0239</th>\n",
       "      <th>Category_007-Product_0234</th>\n",
       "      <th>Category_007-Product_0244</th>\n",
       "      <th>Category_007-Product_0240</th>\n",
       "      <th>Category_007-Product_0233</th>\n",
       "      <th>Category_019-Product_2151</th>\n",
       "      <th>Category_021-Product_0853</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year_Month</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>1590</td>\n",
       "      <td>240</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>222</td>\n",
       "      <td>64600</td>\n",
       "      <td>4000</td>\n",
       "      <td>218</td>\n",
       "      <td>330</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2012-02-01</td>\n",
       "      <td>13900</td>\n",
       "      <td>170</td>\n",
       "      <td>16000</td>\n",
       "      <td>10</td>\n",
       "      <td>227</td>\n",
       "      <td>86000</td>\n",
       "      <td>5000</td>\n",
       "      <td>34</td>\n",
       "      <td>620</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2012-03-01</td>\n",
       "      <td>37695</td>\n",
       "      <td>210</td>\n",
       "      <td>6000</td>\n",
       "      <td>27</td>\n",
       "      <td>499</td>\n",
       "      <td>59600</td>\n",
       "      <td>6000</td>\n",
       "      <td>54</td>\n",
       "      <td>560</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2012-04-01</td>\n",
       "      <td>2608</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>594</td>\n",
       "      <td>57100</td>\n",
       "      <td>7000</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2012-05-01</td>\n",
       "      <td>5968</td>\n",
       "      <td>140</td>\n",
       "      <td>2000</td>\n",
       "      <td>12</td>\n",
       "      <td>372</td>\n",
       "      <td>49600</td>\n",
       "      <td>4000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1625 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Category_007-Product_0138  Category_021-Product_0282  \\\n",
       "Year_Month                                                         \n",
       "2012-01-01                       1590                        240   \n",
       "2012-02-01                      13900                        170   \n",
       "2012-03-01                      37695                        210   \n",
       "2012-04-01                       2608                        110   \n",
       "2012-05-01                       5968                        140   \n",
       "\n",
       "            Category_006-Product_0107  Category_001-Product_0609  \\\n",
       "Year_Month                                                         \n",
       "2012-01-01                          0                         11   \n",
       "2012-02-01                      16000                         10   \n",
       "2012-03-01                       6000                         27   \n",
       "2012-04-01                          0                         10   \n",
       "2012-05-01                       2000                         12   \n",
       "\n",
       "            Category_007-Product_0322  Category_005-Product_0031  \\\n",
       "Year_Month                                                         \n",
       "2012-01-01                        222                      64600   \n",
       "2012-02-01                        227                      86000   \n",
       "2012-03-01                        499                      59600   \n",
       "2012-04-01                        594                      57100   \n",
       "2012-05-01                        372                      49600   \n",
       "\n",
       "            Category_019-Product_1228  Category_024-Product_0289  \\\n",
       "Year_Month                                                         \n",
       "2012-01-01                       4000                        218   \n",
       "2012-02-01                       5000                         34   \n",
       "2012-03-01                       6000                         54   \n",
       "2012-04-01                       7000                         21   \n",
       "2012-05-01                       4000                         10   \n",
       "\n",
       "            Category_021-Product_0277  Category_024-Product_1898  ...  \\\n",
       "Year_Month                                                        ...   \n",
       "2012-01-01                        330                         13  ...   \n",
       "2012-02-01                        620                          8  ...   \n",
       "2012-03-01                        560                         37  ...   \n",
       "2012-04-01                         10                          0  ...   \n",
       "2012-05-01                         10                          3  ...   \n",
       "\n",
       "            Category_007-Product_0241  Category_007-Product_0242  \\\n",
       "Year_Month                                                         \n",
       "2012-01-01                          0                          0   \n",
       "2012-02-01                          0                          0   \n",
       "2012-03-01                          0                          0   \n",
       "2012-04-01                          0                          0   \n",
       "2012-05-01                          0                          0   \n",
       "\n",
       "            Category_007-Product_0238  Category_007-Product_0239  \\\n",
       "Year_Month                                                         \n",
       "2012-01-01                          0                          0   \n",
       "2012-02-01                          0                          0   \n",
       "2012-03-01                          0                          0   \n",
       "2012-04-01                          0                          0   \n",
       "2012-05-01                          0                          0   \n",
       "\n",
       "            Category_007-Product_0234  Category_007-Product_0244  \\\n",
       "Year_Month                                                         \n",
       "2012-01-01                          0                          0   \n",
       "2012-02-01                          0                          0   \n",
       "2012-03-01                          0                          0   \n",
       "2012-04-01                          0                          0   \n",
       "2012-05-01                          0                          0   \n",
       "\n",
       "            Category_007-Product_0240  Category_007-Product_0233  \\\n",
       "Year_Month                                                         \n",
       "2012-01-01                          0                          0   \n",
       "2012-02-01                          0                          0   \n",
       "2012-03-01                          0                          0   \n",
       "2012-04-01                          0                          0   \n",
       "2012-05-01                          0                          0   \n",
       "\n",
       "            Category_019-Product_2151  Category_021-Product_0853  \n",
       "Year_Month                                                        \n",
       "2012-01-01                          0                          0  \n",
       "2012-02-01                          0                          0  \n",
       "2012-03-01                          0                          0  \n",
       "2012-04-01                          0                          0  \n",
       "2012-05-01                          0                          0  \n",
       "\n",
       "[5 rows x 1625 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('Whse_J_sort_out_Month_aggressive.csv')\n",
    "df_test.columns.values[0] = 'Year_Month'\n",
    "df_test.index = pd.to_datetime(df_test['Year_Month'])\n",
    "df_test.drop('Year_Month', axis=1,inplace=True)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive forecast  單變數的序列預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naive forecast－單變數的序列預測\n",
    "# convert time series into supervised learning problem\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1] #單變數為１，多變數為shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    \n",
    "    #generate t-1 col to forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]    \n",
    "        \n",
    "    #generate t, t+1, t+2 cols\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    print(agg)\n",
    "    return agg\n",
    " \n",
    "# transform series into train and test sets for supervised learning\n",
    "def prepare_data(series, n_test, n_lag, n_seq):\n",
    "    # extract raw values\n",
    "    raw_values = series.values\n",
    "    #由於shift需要是２Ｄ資料輸入\n",
    "    raw_values = raw_values.reshape(len(raw_values), 1)\n",
    "    # transform into supervised learning problem X, y\n",
    "    supervised = series_to_supervised(raw_values, n_lag, n_seq)\n",
    "    supervised_values = supervised.values\n",
    "    # split into train and test sets\n",
    "    train, test = supervised_values[0:-n_test], supervised_values[-n_test:]\n",
    "    return train, test\n",
    " \n",
    "# make a persistence forecast\n",
    "def persistence(last_ob, n_seq):\n",
    "    return [last_ob for i in range(n_seq)]\n",
    " \n",
    "# evaluate the persistence model\n",
    "def make_forecasts(train, test, n_lag, n_seq):\n",
    "    forecasts = list()\n",
    "    for i in range(len(test)):\n",
    "        X, y = test[i, 0:n_lag], test[i, n_lag:]\n",
    "        # make forecast\n",
    "        forecast = persistence(X[-1], n_seq)\n",
    "        # store the forecast\n",
    "        forecasts.append(forecast)\n",
    "    return forecasts\n",
    " \n",
    "# evaluate the SMAPE for each forecast time step\n",
    "epsilon = 0.1\n",
    "def evaluate_forecasts(test, forecasts, n_lag, n_seq):\n",
    "    for i in range(n_seq):\n",
    "        actual = test[:,(n_lag+i)]\n",
    "        print('   actual:  ',actual)\n",
    "        predicted = [forecast[i] for forecast in forecasts]\n",
    "        print('predicted:',predicted)\n",
    "        summ = np.maximum(np.abs(actual)+np.abs(predicted)+0.1, 0.5+0.1)\n",
    "        smape = np.mean(np.abs(predicted - actual) / summ) * 200.0\n",
    "        print('t+%d SMAPE: %f' % ((i+1), smape))\n",
    "        print('-'*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category_007-Product_0138</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year_Month</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>1590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2012-02-01</td>\n",
       "      <td>13900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2012-03-01</td>\n",
       "      <td>37695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2012-04-01</td>\n",
       "      <td>2608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2012-05-01</td>\n",
       "      <td>5968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2012-06-01</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>1061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2012-08-01</td>\n",
       "      <td>1512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2012-10-01</td>\n",
       "      <td>2857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2012-11-01</td>\n",
       "      <td>7139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2012-12-01</td>\n",
       "      <td>7574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2013-02-01</td>\n",
       "      <td>1227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>1373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2013-04-01</td>\n",
       "      <td>1561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2013-05-01</td>\n",
       "      <td>3505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2013-06-01</td>\n",
       "      <td>4662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>2327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2013-08-01</td>\n",
       "      <td>1747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Category_007-Product_0138\n",
       "Year_Month                           \n",
       "2012-01-01                       1590\n",
       "2012-02-01                      13900\n",
       "2012-03-01                      37695\n",
       "2012-04-01                       2608\n",
       "2012-05-01                       5968\n",
       "2012-06-01                       2022\n",
       "2012-07-01                       1061\n",
       "2012-08-01                       1512\n",
       "2012-09-01                        410\n",
       "2012-10-01                       2857\n",
       "2012-11-01                       7139\n",
       "2012-12-01                       7574\n",
       "2013-01-01                       2991\n",
       "2013-02-01                       1227\n",
       "2013-03-01                       1373\n",
       "2013-04-01                       1561\n",
       "2013-05-01                       3505\n",
       "2013-06-01                       4662\n",
       "2013-07-01                       2327\n",
       "2013-08-01                       1747"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose the Category_007-Product_0138 for the product to predict\n",
    "series = df_test[['Category_007-Product_0138']]\n",
    "series[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    var1(t-1)  var1(t)  var1(t+1)  var1(t+2)\n",
      "1      1590.0    13900    37695.0     2608.0\n",
      "2     13900.0    37695     2608.0     5968.0\n",
      "3     37695.0     2608     5968.0     2022.0\n",
      "4      2608.0     5968     2022.0     1061.0\n",
      "5      5968.0     2022     1061.0     1512.0\n",
      "6      2022.0     1061     1512.0      410.0\n",
      "7      1061.0     1512      410.0     2857.0\n",
      "8      1512.0      410     2857.0     7139.0\n",
      "9       410.0     2857     7139.0     7574.0\n",
      "10     2857.0     7139     7574.0     2991.0\n",
      "11     7139.0     7574     2991.0     1227.0\n",
      "12     7574.0     2991     1227.0     1373.0\n",
      "13     2991.0     1227     1373.0     1561.0\n",
      "14     1227.0     1373     1561.0     3505.0\n",
      "15     1373.0     1561     3505.0     4662.0\n",
      "16     1561.0     3505     4662.0     2327.0\n",
      "17     3505.0     4662     2327.0     1747.0\n",
      "18     4662.0     2327     1747.0     2207.0\n",
      "19     2327.0     1747     2207.0     1723.0\n",
      "20     1747.0     2207     1723.0     2956.0\n",
      "21     2207.0     1723     2956.0     3844.0\n",
      "22     1723.0     2956     3844.0     1745.0\n",
      "23     2956.0     3844     1745.0     2153.0\n",
      "24     3844.0     1745     2153.0     3436.0\n",
      "25     1745.0     2153     3436.0      947.0\n",
      "26     2153.0     3436      947.0     1549.0\n",
      "27     3436.0      947     1549.0     2213.0\n",
      "28      947.0     1549     2213.0     1651.0\n",
      "29     1549.0     2213     1651.0     4046.0\n",
      "30     2213.0     1651     4046.0     2110.0\n",
      "31     1651.0     4046     2110.0     4582.0\n",
      "32     4046.0     2110     4582.0     2774.0\n",
      "33     2110.0     4582     2774.0     4224.0\n",
      "34     4582.0     2774     4224.0     1087.0\n",
      "35     2774.0     4224     1087.0     1858.0\n",
      "36     4224.0     1087     1858.0     4203.0\n",
      "37     1087.0     1858     4203.0     2189.0\n",
      "38     1858.0     4203     2189.0      635.0\n",
      "39     4203.0     2189      635.0      940.0\n",
      "40     2189.0      635      940.0      690.0\n",
      "41      635.0      940      690.0     1601.0\n",
      "42      940.0      690     1601.0      243.0\n",
      "43      690.0     1601      243.0     3104.0\n",
      "44     1601.0      243     3104.0      785.0\n",
      "45      243.0     3104      785.0     2599.0\n",
      "46     3104.0      785     2599.0     1401.0\n",
      "47      785.0     2599     1401.0     1789.0\n",
      "48     2599.0     1401     1789.0     2468.0\n",
      "49     1401.0     1789     2468.0     1249.0\n",
      "50     1789.0     2468     1249.0     5229.0\n",
      "51     2468.0     1249     5229.0     1231.0\n",
      "52     1249.0     5229     1231.0     2638.0\n",
      "53     5229.0     1231     2638.0     3220.0\n",
      "54     1231.0     2638     3220.0      516.0\n",
      "55     2638.0     3220      516.0     4414.0\n",
      "56     3220.0      516     4414.0     2774.0\n",
      "57      516.0     4414     2774.0     3120.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2599., 1401., 1789., 2468.],\n",
       "       [1401., 1789., 2468., 1249.],\n",
       "       [1789., 2468., 1249., 5229.],\n",
       "       [2468., 1249., 5229., 1231.],\n",
       "       [1249., 5229., 1231., 2638.],\n",
       "       [5229., 1231., 2638., 3220.],\n",
       "       [1231., 2638., 3220.,  516.],\n",
       "       [2638., 3220.,  516., 4414.],\n",
       "       [3220.,  516., 4414., 2774.],\n",
       "       [ 516., 4414., 2774., 3120.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# configure\n",
    "n_lag = 1\n",
    "n_seq = 3\n",
    "n_test = 10\n",
    "# prepare data\n",
    "train, test = prepare_data(series, n_test, n_lag, n_seq)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make forecasts\n",
    "#[[2599.0, 2599.0, 2599.0],..做出３個預測欄位，之後要與３個時間序列真實值比較\n",
    "forecasts = make_forecasts(train, test, n_lag, n_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2599.0, 2599.0, 2599.0],\n",
       " [1401.0, 1401.0, 1401.0],\n",
       " [1789.0, 1789.0, 1789.0],\n",
       " [2468.0, 2468.0, 2468.0],\n",
       " [1249.0, 1249.0, 1249.0],\n",
       " [5229.0, 5229.0, 5229.0],\n",
       " [1231.0, 1231.0, 1231.0],\n",
       " [2638.0, 2638.0, 2638.0],\n",
       " [3220.0, 3220.0, 3220.0],\n",
       " [516.0, 516.0, 516.0]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   actual:   [1401. 1789. 2468. 1249. 5229. 1231. 2638. 3220.  516. 4414.]\n",
      "predicted: [2599.0, 1401.0, 1789.0, 2468.0, 1249.0, 5229.0, 1231.0, 2638.0, 3220.0, 516.0]\n",
      "t+1 SMAPE: 82.384344\n",
      "----------------------------------------------------------------------\n",
      "   actual:   [1789. 2468. 1249. 5229. 1231. 2638. 3220.  516. 4414. 2774.]\n",
      "predicted: [2599.0, 1401.0, 1789.0, 2468.0, 1249.0, 5229.0, 1231.0, 2638.0, 3220.0, 516.0]\n",
      "t+2 SMAPE: 65.915078\n",
      "----------------------------------------------------------------------\n",
      "   actual:   [2468. 1249. 5229. 1231. 2638. 3220.  516. 4414. 2774. 3120.]\n",
      "predicted: [2599.0, 1401.0, 1789.0, 2468.0, 1249.0, 5229.0, 1231.0, 2638.0, 3220.0, 516.0]\n",
      "t+3 SMAPE: 59.090734\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# evaluate forecasts\n",
    "evaluate_forecasts(test, forecasts, n_lag, n_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert time series into supervised learning problem\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    " \n",
    "\n",
    "#transform series into train and test sets for supervised learning\n",
    "def prepare_data(series, n_test, n_lag, n_seq):\n",
    "    # extract raw values\n",
    "    raw_values = series.values\n",
    "    diff_values = raw_values.reshape(len(raw_values), 1)\n",
    "    # rescale values to 0, 1\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_values = scaler.fit_transform(diff_values)\n",
    "    scaled_values = scaled_values.reshape(len(scaled_values), 1)\n",
    "    # transform into supervised learning problem X, y\n",
    "    supervised = series_to_supervised(scaled_values, n_lag, n_seq)\n",
    "    supervised_values = supervised.values\n",
    "    # split into train and test sets\n",
    "    train, test = supervised_values[0:-n_test], supervised_values[-n_test:]\n",
    "    return scaler, train, test\n",
    "\n",
    "# Define SMAPE loss function\n",
    "def customLoss(true,predicted):\n",
    "    epsilon = 0.1\n",
    "    summ = K.maximum(K.abs(true) + K.abs(predicted) + epsilon, 0.5 + epsilon)\n",
    "    smape = K.mean(K.abs(predicted - true) / summ) * 200.0\n",
    "    return smape\n",
    " \n",
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, n_lag, n_seq, n_batch, nb_epoch, n_neurons):\n",
    "    # reshape training into [samples, timesteps, features]\n",
    "    X, y = train[:, 0:n_lag], train[:, n_lag:]\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_neurons ,batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True))\n",
    "    model.add(Dense(y.shape[1]))\n",
    "\n",
    "    model.compile(loss=customLoss, optimizer='adam')\n",
    "    \n",
    "    # fit network\n",
    "    loss=list()\n",
    "    val_loss=list()\n",
    "    for i in range(nb_epoch):\n",
    "        history=model.fit(X, y, epochs=1, batch_size=n_batch, verbose=1,validation_split=0.25, shuffle=False)\n",
    "        eqm=history.history['loss']\n",
    "        eqm_val=history.history['val_loss']\n",
    "        loss.append(eqm)\n",
    "        val_loss.append(eqm_val)\n",
    "        model.reset_states()\n",
    "    return  model,loss,val_loss\n",
    " \n",
    "# make one forecast with an LSTM\n",
    "def forecast_lstm(model, X, n_batch):\n",
    "    # reshape input pattern to [samples, timesteps, features]\n",
    "    print('forecast:X_in.shape:',X.shape)\n",
    "    X = X.reshape(1, 1, len(X))\n",
    "    print('forecast:X_reshape:',X.shape)\n",
    "    # make forecast\n",
    "    forecast = model.predict(X, batch_size=n_batch)\n",
    "    print('forecast:',forecast)\n",
    "    print('forecast:',forecast.shape)\n",
    "    # convert to array\n",
    "    return [x for x in forecast[0, :]]\n",
    " \n",
    "# evaluate the persistence model\n",
    "def make_forecasts(model, n_batch, train, test, n_lag, n_seq):\n",
    "    forecasts = list()\n",
    "    for i in range(len(test)):\n",
    "        X, y = test[i, 0:n_lag], test[i, n_lag:]\n",
    "        # make forecast\n",
    "        forecast = forecast_lstm(model, X, n_batch)\n",
    "        # store the forecast\n",
    "        forecasts.append(forecast)\n",
    "    return forecasts\n",
    " \n",
    "# invert differenced forecast\n",
    "def inverse_difference(last_ob, forecast):\n",
    "    # invert first forecast\n",
    "    inverted = list()\n",
    "    inverted.append(forecast[0] + last_ob)\n",
    "    # propagate difference forecast using inverted first value\n",
    "    for i in range(1, len(forecast)):\n",
    "        inverted.append(forecast[i] + inverted[i-1])\n",
    "    return inverted\n",
    " \n",
    "# inverse data transform on forecasts\n",
    "def inverse_transform(series, forecasts, scaler, n_test):\n",
    "    inverted = list()\n",
    "    for i in range(len(forecasts)):\n",
    "        # create array from forecast\n",
    "        forecast = np.array(forecasts[i])\n",
    "        forecast = forecast.reshape(1, len(forecast))\n",
    "        # invert scaling\n",
    "        inv_scale = scaler.inverse_transform(forecast)\n",
    "        inv_scale = inv_scale[0, :].tolist()\n",
    "        print('inv_scale:',inv_scale)       \n",
    "        # invert differencing\n",
    "        index = len(series) - n_test + i - 1\n",
    "        last_ob = series.values[index]\n",
    "        inverted.append(inv_scale)\n",
    "    return inverted\n",
    " \n",
    "# evaluate the RMSE for each forecast time step\n",
    "epsilon = 0.1\n",
    "def evaluate_forecasts(test, forecasts, n_lag, n_seq):\n",
    "    for i in range(n_seq):\n",
    "        actual = [row[i] for row in test]\n",
    "        print('   actual:  ',actual)\n",
    "        predicted = [forecast[i] for forecast in forecasts]\n",
    "        print('predicted:',predicted)\n",
    "        summ = np.maximum(np.abs(actual)+np.abs(predicted)+0.1, 0.5+0.1)\n",
    "        smape = np.mean(np.abs(np.subtract(predicted,actual)) / summ) * 200.0\n",
    "        print('t+%d SMAPE: %f' % ((i+1), smape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load dataset　　and choose the 'Category_007-Product_0138' for our product to forecast\n",
    "series = df_test['Category_007-Product_0138']\n",
    "# series[-12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure\n",
    "n_lag = 1\n",
    "n_seq = 3\n",
    "n_test = 10\n",
    "n_epochs = 100\n",
    "n_batch = 1\n",
    "n_neurons = 4\n",
    "# prepare data\n",
    "scaler, train, test = prepare_data(series, n_test, n_lag, n_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 1s 41ms/step - loss: 19.0847 - val_loss: 9.4149\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 16.0642 - val_loss: 10.5095\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.9640 - val_loss: 10.0578\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.8163 - val_loss: 9.7036\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.7832 - val_loss: 9.5044\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.7013 - val_loss: 9.4742\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.7235 - val_loss: 9.4664\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.6565 - val_loss: 9.3792\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.6502 - val_loss: 9.4519\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.5940 - val_loss: 9.3490\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.5363 - val_loss: 9.3895\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.5587 - val_loss: 9.5445\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.5515 - val_loss: 9.5229\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.4860 - val_loss: 9.2826\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.4671 - val_loss: 9.3567\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.4714 - val_loss: 9.4137\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.4346 - val_loss: 9.4153\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.4132 - val_loss: 9.4009\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.4128 - val_loss: 9.4414\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.3509 - val_loss: 9.2743\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.3673 - val_loss: 9.4814\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.3445 - val_loss: 9.4292\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.3229 - val_loss: 9.4255\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.2771 - val_loss: 9.5051\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.2970 - val_loss: 9.4577\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.2475 - val_loss: 9.4928\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.2539 - val_loss: 9.4514\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.1979 - val_loss: 9.3830\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.1933 - val_loss: 9.4930\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.1947 - val_loss: 9.3660\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.1576 - val_loss: 9.5217\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.1264 - val_loss: 9.4928\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.1297 - val_loss: 9.4650\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.0857 - val_loss: 9.4714\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.0946 - val_loss: 9.4948\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.0286 - val_loss: 9.4610\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.0523 - val_loss: 9.3813\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.0123 - val_loss: 9.5284\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 15.0112 - val_loss: 9.5421\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.9966 - val_loss: 9.3296\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.9884 - val_loss: 9.4979\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.9963 - val_loss: 9.5002\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.9489 - val_loss: 9.5328\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.9560 - val_loss: 9.4183\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.9463 - val_loss: 9.4566\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.9215 - val_loss: 9.5518\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.9431 - val_loss: 9.5150\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.9017 - val_loss: 9.6459\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.8899 - val_loss: 9.4172\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.9140 - val_loss: 9.5248\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.8846 - val_loss: 9.4157\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.8528 - val_loss: 9.4993\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.8749 - val_loss: 9.6515\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.8375 - val_loss: 9.4612\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.8871 - val_loss: 9.7099\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.8321 - val_loss: 9.5053\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.8703 - val_loss: 9.6506\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.8403 - val_loss: 9.6138\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.8173 - val_loss: 9.5704\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.8322 - val_loss: 9.4882\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.7785 - val_loss: 9.5748\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.7897 - val_loss: 9.6258\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.7901 - val_loss: 9.5610\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.7310 - val_loss: 9.4897\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.8157 - val_loss: 9.5409\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.7488 - val_loss: 9.6341\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.7578 - val_loss: 9.5827\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.7223 - val_loss: 9.5909\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.7077 - val_loss: 9.6421\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.7334 - val_loss: 9.5346\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.6924 - val_loss: 9.5235\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.7238 - val_loss: 9.8041\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.6675 - val_loss: 9.5234\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.6630 - val_loss: 9.6229\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.6564 - val_loss: 9.6815\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.6580 - val_loss: 9.5390\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.6236 - val_loss: 9.6742\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.6331 - val_loss: 9.7506\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.6219 - val_loss: 9.7287\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.5992 - val_loss: 9.6843\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.5808 - val_loss: 9.5867\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.6304 - val_loss: 9.8616\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.5454 - val_loss: 9.5690\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.5966 - val_loss: 9.6979\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.5800 - val_loss: 9.7630\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.5429 - val_loss: 9.5806\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.5968 - val_loss: 9.8537\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.5205 - val_loss: 9.5291\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.5501 - val_loss: 9.6831\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.5424 - val_loss: 9.6971\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.5371 - val_loss: 9.6179\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.5223 - val_loss: 9.7298\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.5107 - val_loss: 9.6887\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.5199 - val_loss: 9.9298\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.4896 - val_loss: 9.6756\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.5387 - val_loss: 9.8532\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.4693 - val_loss: 9.6898\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.4970 - val_loss: 9.7757\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.4752 - val_loss: 9.8824\n",
      "Train on 35 samples, validate on 12 samples\n",
      "Epoch 1/1\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 14.4865 - val_loss: 9.9147\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fit model\n",
    "model,loss,val_loss= fit_lstm(train, n_lag, n_seq, n_batch, n_epochs, n_neurons)\n",
    "pyplot.figure()\n",
    "pyplot.plot(loss)\n",
    "pyplot.plot(val_loss)\n",
    "pyplot.title('cross validation')\n",
    "pyplot.ylabel('SMAPE')\n",
    "pyplot.xlabel('epoch')\n",
    "pyplot.legend(['training', 'test'], loc='upper left')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forecast:X_in.shape: (1,)\n",
      "forecast:X_reshape: (1, 1, 1)\n",
      "forecast: [[0.04152905 0.05383682 0.06029691]]\n",
      "forecast: (1, 3)\n",
      "forecast:X_in.shape: (1,)\n",
      "forecast:X_reshape: (1, 1, 1)\n",
      "forecast: [[0.04520202 0.05786103 0.06406526]]\n",
      "forecast: (1, 3)\n",
      "forecast:X_in.shape: (1,)\n",
      "forecast:X_reshape: (1, 1, 1)\n",
      "forecast: [[0.04764581 0.06012863 0.06592368]]\n",
      "forecast: (1, 3)\n",
      "forecast:X_in.shape: (1,)\n",
      "forecast:X_reshape: (1, 1, 1)\n",
      "forecast: [[0.04928239 0.061477   0.06629063]]\n",
      "forecast: (1, 3)\n",
      "forecast:X_in.shape: (1,)\n",
      "forecast:X_reshape: (1, 1, 1)\n",
      "forecast: [[0.04988615 0.05996379 0.06646252]]\n",
      "forecast: (1, 3)\n",
      "forecast:X_in.shape: (1,)\n",
      "forecast:X_reshape: (1, 1, 1)\n",
      "forecast: [[0.05144271 0.06385515 0.06464992]]\n",
      "forecast: (1, 3)\n",
      "forecast:X_in.shape: (1,)\n",
      "forecast:X_reshape: (1, 1, 1)\n",
      "forecast: [[0.05059536 0.05857976 0.06361532]]\n",
      "forecast: (1, 3)\n",
      "forecast:X_in.shape: (1,)\n",
      "forecast:X_reshape: (1, 1, 1)\n",
      "forecast: [[0.0510329  0.05845622 0.06285632]]\n",
      "forecast: (1, 3)\n",
      "forecast:X_in.shape: (1,)\n",
      "forecast:X_reshape: (1, 1, 1)\n",
      "forecast: [[0.0513288  0.05833282 0.06174605]]\n",
      "forecast: (1, 3)\n",
      "forecast:X_in.shape: (1,)\n",
      "forecast:X_reshape: (1, 1, 1)\n",
      "forecast: [[0.05066486 0.05437311 0.06181465]]\n",
      "forecast: (1, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.041529052, 0.053836815, 0.060296908],\n",
       " [0.04520202, 0.057861034, 0.06406526],\n",
       " [0.04764581, 0.06012863, 0.065923676],\n",
       " [0.049282387, 0.061477, 0.06629063],\n",
       " [0.049886152, 0.059963785, 0.066462524],\n",
       " [0.051442705, 0.06385515, 0.06464992],\n",
       " [0.050595358, 0.058579758, 0.06361532],\n",
       " [0.051032905, 0.05845622, 0.06285632],\n",
       " [0.0513288, 0.058332823, 0.061746046],\n",
       " [0.050664864, 0.054373108, 0.061814647]]"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make forecasts\n",
    "forecasts = make_forecasts(model, n_batch, train, test, n_lag, n_seq)\n",
    "forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inv_scale: [1798.3460693359375, 2259.29638671875, 2501.239990234375]\n",
      "inv_scale: [1935.906005859375, 2410.011474609375, 2642.372314453125]\n",
      "inv_scale: [2027.430908203125, 2494.9375, 2711.9736328125]\n",
      "inv_scale: [2088.723876953125, 2545.436767578125, 2725.716796875]\n",
      "inv_scale: [2111.336181640625, 2488.763916015625, 2732.154541015625]\n",
      "inv_scale: [2169.632080078125, 2634.503173828125, 2664.268798828125]\n",
      "inv_scale: [2137.897216796875, 2436.92919921875, 2625.521240234375]\n",
      "inv_scale: [2154.284423828125, 2432.302490234375, 2597.094970703125]\n",
      "inv_scale: [2165.3662109375, 2427.680908203125, 2555.512939453125]\n",
      "inv_scale: [2140.50048828125, 2279.381591796875, 2558.08203125]\n",
      "forecasts: [[1798.3460693359375, 2259.29638671875, 2501.239990234375], [1935.906005859375, 2410.011474609375, 2642.372314453125], [2027.430908203125, 2494.9375, 2711.9736328125], [2088.723876953125, 2545.436767578125, 2725.716796875], [2111.336181640625, 2488.763916015625, 2732.154541015625], [2169.632080078125, 2634.503173828125, 2664.268798828125], [2137.897216796875, 2436.92919921875, 2625.521240234375], [2154.284423828125, 2432.302490234375, 2597.094970703125], [2165.3662109375, 2427.680908203125, 2555.512939453125], [2140.50048828125, 2279.381591796875, 2558.08203125]]\n",
      "actual0: [array([0.03091958, 0.0412795 , 0.05940938]), array([0.0412795 , 0.05940938, 0.02686105]), array([0.05940938, 0.02686105, 0.13313041]), array([0.02686105, 0.13313041, 0.02638043]), array([0.13313041, 0.02638043, 0.06394852]), array([0.02638043, 0.06394852, 0.07948841]), array([0.06394852, 0.07948841, 0.00728933]), array([0.07948841, 0.00728933, 0.11136922]), array([0.00728933, 0.11136922, 0.06757984]), array([0.11136922, 0.06757984, 0.07681833])]\n",
      "inv_scale: [1401.0, 1789.0, 2468.0]\n",
      "inv_scale: [1789.0, 2468.0, 1248.9999999999998]\n",
      "inv_scale: [2468.0, 1248.9999999999998, 5229.0]\n",
      "inv_scale: [1248.9999999999998, 5229.0, 1231.0]\n",
      "inv_scale: [5229.0, 1231.0, 2638.0]\n",
      "inv_scale: [1231.0, 2638.0, 3220.0000000000005]\n",
      "inv_scale: [2638.0, 3220.0000000000005, 516.0]\n",
      "inv_scale: [3220.0000000000005, 516.0, 4414.0]\n",
      "inv_scale: [516.0, 4414.0, 2774.0]\n",
      "inv_scale: [4414.0, 2774.0, 3120.0]\n",
      "actual1: [[1401.0, 1789.0, 2468.0], [1789.0, 2468.0, 1248.9999999999998], [2468.0, 1248.9999999999998, 5229.0], [1248.9999999999998, 5229.0, 1231.0], [5229.0, 1231.0, 2638.0], [1231.0, 2638.0, 3220.0000000000005], [2638.0, 3220.0000000000005, 516.0], [3220.0000000000005, 516.0, 4414.0], [516.0, 4414.0, 2774.0], [4414.0, 2774.0, 3120.0]]\n"
     ]
    }
   ],
   "source": [
    "# inverse transform forecasts and test\n",
    "forecasts = inverse_transform(series, forecasts, scaler, n_test+2)\n",
    "print('forecasts:',forecasts)\n",
    "actual = [row[n_lag:] for row in test]\n",
    "print('actual0:',actual)\n",
    "actual = inverse_transform(series, actual, scaler, n_test+2)\n",
    "print('actual1:',actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   actual:   [1401.0, 1789.0, 2468.0, 1248.9999999999998, 5229.0, 1231.0, 2638.0, 3220.0000000000005, 516.0, 4414.0]\n",
      "predicted: [1798.3460693359375, 1935.906005859375, 2027.430908203125, 2088.723876953125, 2111.336181640625, 2169.632080078125, 2137.897216796875, 2154.284423828125, 2165.3662109375, 2140.50048828125]\n",
      "t+1 SMAPE: 49.578089\n",
      "   actual:   [1789.0, 2468.0, 1248.9999999999998, 5229.0, 1231.0, 2638.0, 3220.0000000000005, 516.0, 4414.0, 2774.0]\n",
      "predicted: [2259.29638671875, 2410.011474609375, 2494.9375, 2545.436767578125, 2488.763916015625, 2634.503173828125, 2436.92919921875, 2432.302490234375, 2427.680908203125, 2279.381591796875]\n",
      "t+2 SMAPE: 46.427237\n",
      "   actual:   [2468.0, 1248.9999999999998, 5229.0, 1231.0, 2638.0, 3220.0000000000005, 516.0, 4414.0, 2774.0, 3120.0]\n",
      "predicted: [2501.239990234375, 2642.372314453125, 2711.9736328125, 2725.716796875, 2732.154541015625, 2664.268798828125, 2625.521240234375, 2597.094970703125, 2555.512939453125, 2558.08203125]\n",
      "t+3 SMAPE: 44.840328\n",
      "off_s: 47\n",
      "off_e: 51\n",
      "xaxis: [47, 48, 49, 50]\n",
      "yaxis: [2599, 1798.3460693359375, 2259.29638671875, 2501.239990234375]\n",
      "off_s: 48\n",
      "off_e: 52\n",
      "xaxis: [48, 49, 50, 51]\n",
      "yaxis: [1401, 1935.906005859375, 2410.011474609375, 2642.372314453125]\n",
      "off_s: 49\n",
      "off_e: 53\n",
      "xaxis: [49, 50, 51, 52]\n",
      "yaxis: [1789, 2027.430908203125, 2494.9375, 2711.9736328125]\n",
      "off_s: 50\n",
      "off_e: 54\n",
      "xaxis: [50, 51, 52, 53]\n",
      "yaxis: [2468, 2088.723876953125, 2545.436767578125, 2725.716796875]\n",
      "off_s: 51\n",
      "off_e: 55\n",
      "xaxis: [51, 52, 53, 54]\n",
      "yaxis: [1249, 2111.336181640625, 2488.763916015625, 2732.154541015625]\n",
      "off_s: 52\n",
      "off_e: 56\n",
      "xaxis: [52, 53, 54, 55]\n",
      "yaxis: [5229, 2169.632080078125, 2634.503173828125, 2664.268798828125]\n",
      "off_s: 53\n",
      "off_e: 57\n",
      "xaxis: [53, 54, 55, 56]\n",
      "yaxis: [1231, 2137.897216796875, 2436.92919921875, 2625.521240234375]\n",
      "off_s: 54\n",
      "off_e: 58\n",
      "xaxis: [54, 55, 56, 57]\n",
      "yaxis: [2638, 2154.284423828125, 2432.302490234375, 2597.094970703125]\n",
      "off_s: 55\n",
      "off_e: 59\n",
      "xaxis: [55, 56, 57, 58]\n",
      "yaxis: [3220, 2165.3662109375, 2427.680908203125, 2555.512939453125]\n",
      "off_s: 56\n",
      "off_e: 60\n",
      "xaxis: [56, 57, 58, 59]\n",
      "yaxis: [516, 2140.50048828125, 2279.381591796875, 2558.08203125]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate forecasts\n",
    "evaluate_forecasts(actual, forecasts, n_lag, n_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
